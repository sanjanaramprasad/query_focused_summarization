{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4cef5e1-2129-428c-a8a2-672051701995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5b267fc6-541e-4b1e-be26-5ad75b7c5f8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample_natural_questions(filepath, num_sample):\n",
    "    gpt_instruction = 'Answer the question based on the article. Ensure the answer is consistent with the corresponding article. Note that consistency means all information in the answer is supported by the article.'\n",
    "    df_dict = {\n",
    "        'id': [],\n",
    "        'Article': [],\n",
    "        'Query': [],\n",
    "        'References': [],\n",
    "        'Faithful_Query': [],\n",
    "        'GPT_prompt': [],\n",
    "    }\n",
    "    \n",
    "    df = pd.read_csv(filepath)\n",
    "    df = df[df['Faithful_Query'] == 'yes']\n",
    "    df_sample = df.sample(num_sample)\n",
    "    \n",
    "    references_long = [long_ans\n",
    "                              if type(long_ans) is str else None \n",
    "                                  for long_ans in \n",
    "                                       list(df_sample['Long_Answer'].values)]\n",
    "    references_short = [short_ans \n",
    "                                if type(short_ans) is str else None \n",
    "                                    for short_ans in \n",
    "                                        list(df_sample['Short_Answer'].values)]\n",
    "    references = list(zip(references_long, references_short))\n",
    "    \n",
    "            \n",
    "    for idx, row in df_sample.iterrows():\n",
    "        article = row['Article']\n",
    "        query = row['Query']\n",
    "        gpt_prompt = f'Article: {article}\\n{gpt_instruction}\\nQuestion: {query}\\nAnswer:'\n",
    "        \n",
    "        df_dict['Article'] += [article]\n",
    "        df_dict['Query'] += [query]\n",
    "        df_dict['GPT_prompt'] += [gpt_prompt]\n",
    "    \n",
    "    df_dict['id'] += list(df_sample['id'].values)\n",
    "    df_dict['References'] += references\n",
    "    df_dict['Faithful_Query']+= list(df_sample['Faithful_Query'].values)\n",
    "        \n",
    "    print([(k, len(v)) for k, v in df_dict.items()])\n",
    "    return df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cfa31abd-3970-4a8a-8e54-5d65966c9232",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('id', 100), ('Article', 100), ('Query', 100), ('References', 100), ('Faithful_Query', 100), ('GPT_prompt', 100)]\n"
     ]
    }
   ],
   "source": [
    "fileinfo = {\n",
    "    'natural_questions': '/home/ramprasad.sa/qf_sd_summarization/datasets/postprocessed/query_focused/natural_questions.csv',\n",
    "    'adversarial_qa': '/home/ramprasad.sa/qf_sd_summarization/datasets/postprocessed/query_focused/adversarial_qa.csv',\n",
    "    'squality': '/home/ramprasad.sa/qf_sd_summarization/datasets/postprocessed/query_focused/squality.csv'\n",
    "}\n",
    "\n",
    "df = sample_natural_questions(fileinfo['natural_questions'], 100)\n",
    "df = pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "64808be4-d01b-4473-be20-6e5fefe1ed81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv('/home/ramprasad.sa/qf_sd_summarization/datasets/postprocessed/query_focused/natural_questions_sample.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "756e33e0-aab8-48d2-a301-df08fa989067",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>Article</th>\n",
       "      <th>Query</th>\n",
       "      <th>Long_Answer</th>\n",
       "      <th>Short_Answer</th>\n",
       "      <th>Faithful_Query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4286</th>\n",
       "      <td>4286</td>\n",
       "      <td>-6824623915825732866</td>\n",
       "      <td>Figure skating at the Olympic Games - wikipedi...</td>\n",
       "      <td>when is the figure skating part of the olympics</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                   id  \\\n",
       "4286        4286 -6824623915825732866   \n",
       "\n",
       "                                                Article  \\\n",
       "4286  Figure skating at the Olympic Games - wikipedi...   \n",
       "\n",
       "                                                Query Long_Answer  \\\n",
       "4286  when is the figure skating part of the olympics         NaN   \n",
       "\n",
       "     Short_Answer Faithful_Query  \n",
       "4286          NaN            yes  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nq = pd.read_csv(fileinfo['natural_questions'])\n",
    "df_nq[df_nq['id'] == -6824623915825732866]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "246f48f6-19c0-4429-b331-b59e50fa142a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'where did they film high school musical two'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Query'].values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1a7ac8ea-215f-40bd-84bd-3814e54a4c37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def sample_rq1(file_info, num_sample = 100):\n",
    "    df_sample = {\n",
    "    'id': [],\n",
    "    'Article': [],\n",
    "    'Query': [],\n",
    "    'References': [],\n",
    "    'Faithful_Query': []\n",
    "  \n",
    "    }\n",
    "    \n",
    "    for filetype, filepath in file_info.items():\n",
    "        df = pd.read_csv(filepath)\n",
    "        print(filetype, df.keys())\n",
    "        df_sample_ft = df.sample(100)\n",
    "    \n",
    "        if filetype == 'natural_questions':\n",
    "            references_long = [long_ans\n",
    "                              if type(long_ans) is str else None \n",
    "                                  for long_ans in \n",
    "                                       list(df_sample_ft['Long_Answer'].values)]\n",
    "            references_short = [short_ans \n",
    "                                if type(short_ans) is str else None \n",
    "                                    for short_ans in \n",
    "                                        list(df_sample_ft['Short_Answer'].values)]\n",
    "            references = list(zip(references_long, references_short))\n",
    "        \n",
    "        elif filetype == 'adversarial_qa':\n",
    "            references = list(df_sample_ft['Answers'].values)\n",
    "            references = [each if each else None for each in references]\n",
    "            \n",
    "        elif filetype == 'squality':\n",
    "            references = list(df_sample_ft['Answers'].values)\n",
    "            references = [eval(each) for each in references]\n",
    "    \n",
    "        df_sample['id'] += list(df_sample_ft['id'].values)\n",
    "        df_sample['Article'] += list(df_sample_ft['Article'].values)\n",
    "        df_sample['Query'] += list(df_sample_ft['Query'].values)\n",
    "        df_sample['References'] += references\n",
    "        df_sample['Faithful_Query']+= list(df_sample_ft['Faithful_Query'].values)\n",
    "\n",
    "    assert len(set([len(v) for k , v in df_sample_rq1.items()])) == 1\n",
    "    return df_sample\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2b4a5c03-3191-49b4-8080-2aa833afea5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "natural_questions Index(['Unnamed: 0', 'id', 'Article', 'Query', 'Long_Answer', 'Short_Answer',\n",
      "       'Faithful_Query'],\n",
      "      dtype='object')\n",
      "adversarial_qa Index(['Unnamed: 0', 'id', 'Article', 'Query', 'Answers', 'Faithful_Query'], dtype='object')\n",
      "squality Index(['Unnamed: 0', 'id', 'Article', 'Query', 'Answers', 'Faithful_Query'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "df_sample = sample_rq1(fileinfo, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a94810f7-a9a5-43de-978c-7d92c253f108",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pd.read_csv(fileinfo['natural_questions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "388ab2ca-99d5-4bd9-916b-45209ee70fac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Article_OG</th>\n",
       "      <th>Article</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Aspect_Sents</th>\n",
       "      <th>Query</th>\n",
       "      <th>Reference</th>\n",
       "      <th>Faithful_Query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34341</th>\n",
       "      <td>34341</td>\n",
       "      <td>67337651788602441439400781675753097745</td>\n",
       "      <td>Restauration (ship)</td>\n",
       "      <td>On what is considered the first organized emig...</td>\n",
       "      <td>The Norse-American Centennial was held in Minn...</td>\n",
       "      <td>History</td>\n",
       "      <td>On what is considered the first organized emig...</td>\n",
       "      <td>Summarize the History of Restauration (ship)</td>\n",
       "      <td>It became a symbol of Norwegian American immig...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97673</th>\n",
       "      <td>97673</td>\n",
       "      <td>335433622514591354420104586042173081638</td>\n",
       "      <td>Amblyaudia</td>\n",
       "      <td>Children with amblyaudia experience difficulti...</td>\n",
       "      <td>Families report the presence of amblyaudia in ...</td>\n",
       "      <td>Symptoms and signs</td>\n",
       "      <td>Children with amblyaudia experience difficulti...</td>\n",
       "      <td>Summarize the Symptoms and signs of Amblyaudia</td>\n",
       "      <td>Individuals with amblyaudia have normal hearin...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                       id  \\\n",
       "34341       34341   67337651788602441439400781675753097745   \n",
       "97673       97673  335433622514591354420104586042173081638   \n",
       "\n",
       "                     Title                                         Article_OG  \\\n",
       "34341  Restauration (ship)  On what is considered the first organized emig...   \n",
       "97673           Amblyaudia  Children with amblyaudia experience difficulti...   \n",
       "\n",
       "                                                 Article              Aspect  \\\n",
       "34341  The Norse-American Centennial was held in Minn...             History   \n",
       "97673  Families report the presence of amblyaudia in ...  Symptoms and signs   \n",
       "\n",
       "                                            Aspect_Sents  \\\n",
       "34341  On what is considered the first organized emig...   \n",
       "97673  Children with amblyaudia experience difficulti...   \n",
       "\n",
       "                                                Query  \\\n",
       "34341    Summarize the History of Restauration (ship)   \n",
       "97673  Summarize the Symptoms and signs of Amblyaudia   \n",
       "\n",
       "                                               Reference Faithful_Query  \n",
       "34341  It became a symbol of Norwegian American immig...             no  \n",
       "97673  Individuals with amblyaudia have normal hearin...             no  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = '/home/ramprasad.sa/qf_sd_summarization/datasets/postprocessed/aspect_focused/oasumm.csv'\n",
    "df = pd.read_csv(filepath)\n",
    "oasumm_sample = df.sample(100)\n",
    "oasumm_sample.head()[:2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcece056-b01a-40d0-891a-0cb7598a4495",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>Article</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Query</th>\n",
       "      <th>Reference</th>\n",
       "      <th>Faithful_Query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>241</td>\n",
       "      <td>270039784860198903136674953923632489510</td>\n",
       "      <td>Minamisoma, Japan (CNN) -- The scene of sheer ...</td>\n",
       "      <td>geography, region, or location</td>\n",
       "      <td>geography, region, or location</td>\n",
       "      <td>Minamisoma, Japan (CNN) -- The scene of sheer ...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>387</td>\n",
       "      <td>123252535517968928407575531269154458303</td>\n",
       "      <td>(CNN) -- The magnitude-8.8 earthquake that roc...</td>\n",
       "      <td>geography, region, or location</td>\n",
       "      <td>geography, region, or location</td>\n",
       "      <td>(CNN) -- The magnitude-8.8 earthquake that roc...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                       id  \\\n",
       "241         241  270039784860198903136674953923632489510   \n",
       "387         387  123252535517968928407575531269154458303   \n",
       "\n",
       "                                               Article  \\\n",
       "241  Minamisoma, Japan (CNN) -- The scene of sheer ...   \n",
       "387  (CNN) -- The magnitude-8.8 earthquake that roc...   \n",
       "\n",
       "                             Aspect                           Query  \\\n",
       "241  geography, region, or location  geography, region, or location   \n",
       "387  geography, region, or location  geography, region, or location   \n",
       "\n",
       "                                             Reference Faithful_Query  \n",
       "241  Minamisoma, Japan (CNN) -- The scene of sheer ...            yes  \n",
       "387  (CNN) -- The magnitude-8.8 earthquake that roc...            yes  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = '/home/ramprasad.sa/qf_sd_summarization/datasets/postprocessed/aspect_focused/aspect_news_summ.csv'\n",
    "df = pd.read_csv(filepath)\n",
    "aspect_news_summ_sample = df.sample(100)\n",
    "aspect_news_summ_sample.head()[:2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9058a0-436a-49f7-91eb-78767995130d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qfs",
   "language": "python",
   "name": "qfs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
